{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/subornaa/Data-Analytics-Tutorials/blob/main/CART.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "wSsWd0eQl8ac"
      },
      "id": "wSsWd0eQl8ac"
    },
    {
      "cell_type": "markdown",
      "id": "967761e4",
      "metadata": {
        "id": "967761e4"
      },
      "source": [
        "# CART: Classification and Regression Trees\n",
        "<img src = 'https://drive.google.com/uc?id=1CufocdcU3wMg86Sr537-08HUP1C4Reom' width = 80%>\n",
        "\n",
        "*Image source: Louppe, Gilles. Understanding random forests: From theory to practice. Diss. Universite de Liege (Belgium), 2014.*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **_Note:_** Be aware that certain functions, including RFECV, Grid Search, and Random Search, may require significant computation time. We anticipate that each individual execution of these functions will not exceed 5 minutes."
      ],
      "metadata": {
        "id": "tYt1ekKRnyBe"
      },
      "id": "tYt1ekKRnyBe"
    },
    {
      "cell_type": "markdown",
      "id": "11c2751d",
      "metadata": {
        "id": "11c2751d"
      },
      "source": [
        "# Introduction and Dataset\n",
        "\n",
        "## Background\n",
        "\n",
        "This tutorial provides an example of how to apply classification and regression trees (CARTs), specifically random forest (RF) to model tree species. Here we will approach this challenge in two ways 1) classifying dominant tree species; and 2) regressing tree species proportions.\n",
        "\n",
        "Estimating dominant tree species is a common task in forestry that is often performed using remote sensing such as in Fancini et al. (2024).\n",
        "\n",
        "[*Francini, Saverio, et al. \"Forest species mapping and area proportion estimation combining Sentinel-2 harmonic predictors and national forest inventory data.\" International Journal of Applied Earth Observation and Geoinformation 131 (2024): 103935.*](https://doi.org/10.1016/j.jag.2024.103935)\n",
        "\n",
        "## Tutorial goals\n",
        "\n",
        "**Goal 1: Develop a random forest (RF) classification model to classify dominant tree species and tree type (coniferous vs. deciduous)**\n",
        "\n",
        "**Goal 2: Create a new RF model that performs species proportion regression and compare this to the species classification model.**\n",
        "\n",
        "**Goal 3: Perform feature selection using scikit-learn and remove correlated predictors**\n",
        "\n",
        "**Goal 4: Experiment with different hyperparameters to limit overfitting**\n",
        "\n",
        "\n",
        "-----\n",
        "\n",
        "## Data\n",
        "\n",
        "This tutorial makes use of light detection and ranging (LiDAR) metrics and multispectral indices that are derived in the Lasso and Ridge Regression Tutorial notebook. Specifically, the `predictors.csv` file generated contains all the predictor variables used in the current tutorial. Please refer to the Lasso and Ridge Regression tutorial for more information about how these metrics/indices were calculated."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "366a6a13",
      "metadata": {
        "id": "366a6a13"
      },
      "source": [
        "# Install and load packages\n",
        "\n",
        "**Run the cell below to install required packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91fa8ce4",
      "metadata": {
        "id": "91fa8ce4"
      },
      "outputs": [],
      "source": [
        "!pip install -q pandas==2.2.2\n",
        "!pip install -q geopandas==1.0.1\n",
        "!pip install -q matplotlib==3.10.1\n",
        "!pip install -q rioxarray==0.19.0\n",
        "!pip install -q spyndex==0.5.0\n",
        "!pip install -q pyarrow==19.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a3dc65b",
      "metadata": {
        "id": "4a3dc65b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "import numpy as np\n",
        "from numpy import sqrt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70ef55f3",
      "metadata": {
        "id": "70ef55f3"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd1ac756",
      "metadata": {
        "id": "dd1ac756"
      },
      "outputs": [],
      "source": [
        "# Download the data if it does not yet exist\n",
        "if not os.path.exists(\"data\"):\n",
        "  !gdown 1UDKAdXW0h6JSf7k31PZ-srrQ3487l9e2\n",
        "  !unzip prf_data.zip -d data/\n",
        "  os.remove(\"prf_data.zip\")\n",
        "else:\n",
        "  print(\"Data has already been downloaded.\")\n",
        "\n",
        "os.listdir(\"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block of code calculates dominant tree species based on biomass."
      ],
      "metadata": {
        "id": "rsCyaBfb5SKg"
      },
      "id": "rsCyaBfb5SKg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a826255",
      "metadata": {
        "id": "4a826255"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data/predictors.csv\")\n",
        "\n",
        "# Extract predictor var list\n",
        "features_ls = df.drop(\"PlotName\", axis=1).columns.tolist()\n",
        "\n",
        "#Uncomment to see all the variables\n",
        "# print(\"Predictor variables:\")\n",
        "# print(\"\\n\".join(features_ls))\n",
        "\n",
        "trees_df = pd.read_csv(\"data/trees.csv\")\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "959d6174",
      "metadata": {
        "id": "959d6174"
      },
      "source": [
        "# Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us calculate dominant species in each plot based on biomass for our experiment. First we remove trailing whitespace from species names, then we can view the species composition across entire dataset by count."
      ],
      "metadata": {
        "id": "0IN3ttjb_ehL"
      },
      "id": "0IN3ttjb_ehL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0fef3e",
      "metadata": {
        "id": "4a0fef3e"
      },
      "outputs": [],
      "source": [
        "trees_df['species'] = trees_df['species'].str.strip()\n",
        "\n",
        "print(trees_df['species'].value_counts() / len(trees_df) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 - What can be said about the distribution of species? Describe species evenenss, richness, abundance, and overall diversity in this dataset.**"
      ],
      "metadata": {
        "id": "SUG9xAl8ITtv"
      },
      "id": "SUG9xAl8ITtv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer here*"
      ],
      "metadata": {
        "id": "Lyu6UY3cX4xe"
      },
      "id": "Lyu6UY3cX4xe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details open>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "Distribution of Species:\n",
        "\n",
        "- Balsam fir, White pine, Red (soft) maple, Red pine are the most common speices sharing roughly the same amount of biomass in the area.\n",
        "\n",
        "Abundance:\n",
        "\n",
        "- There is a uneven distribution of abundance among the species.\n",
        "\n",
        "- Dominant Species: Balsam fir and White pine are by far the most abundant species, with counts of 14.69 and 14.66 respectively. Red (soft) maple (13.12) is also abundant.\n",
        "\n",
        "- Moderately Abundant Species: Red pine (10.96) and Jack pine (4.74) show moderate abundance.\n",
        "\n",
        "- Low Abundance Species: A significant number of species have very low abundance, particularly those towards the bottom of the list (e.g., Eastern hemlock, White ash, American elm, Black cherry, Balsam poplar) all with values less than 0.5. Balsam poplar is the least abundant at 0.02.\n",
        "\n",
        "Evenness:\n",
        "\n",
        "- The species exhibit low evenness. This is evident from the stark contrast between the high abundance of the top few species and the very low abundance of many others. If the species were evenly distributed, their abundance values would be much closer to each other. The data shows a steep decline in abundance from the most common to the rarest species.\n",
        "\n",
        "Overall Diversity:\n",
        "\n",
        "- While the dataset boasts a decent species *richness* (24 species), the low evenness significantly impacts the overall diversity. High dominance by a few species reduces the effective diversity of the ecosystem or sample represented.\n",
        "\n",
        "In summary, this dataset indicates an ecosystem or sample that is relatively rich in the number of different species, but where a few species are overwhelmingly dominant, leading to low evenness and a lower overall effective diversity than the raw species count might suggest.\n",
        "</details>"
      ],
      "metadata": {
        "id": "K63Q8U5763WX"
      },
      "id": "K63Q8U5763WX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a routine cleanup when working with any dataset, replace NAs in species with an interpertable value, in this case \"Unknown\"."
      ],
      "metadata": {
        "id": "Td_GXf5pIEgC"
      },
      "id": "Td_GXf5pIEgC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0a624f",
      "metadata": {
        "id": "3c0a624f"
      },
      "outputs": [],
      "source": [
        "trees_df['species'] = trees_df['species'].fillna(\"Unknown\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we are done with the preprocessing, time to start exploring our experiment below."
      ],
      "metadata": {
        "id": "xUyb6Hre9i-0"
      },
      "id": "xUyb6Hre9i-0"
    },
    {
      "cell_type": "markdown",
      "id": "1f8ea3c9",
      "metadata": {
        "id": "1f8ea3c9"
      },
      "source": [
        "# Tree Species Dominance\n",
        "\n",
        "There are different ways of determining the dominant (i.e., leading) tree species in a given area. The simplest approach perhaps is to count the number of trees of a given species. However, this may not accurately represent dominance since one species may have many small trees that do not account for much volume or area.\n",
        "\n",
        "Another approach is to calculate dominance based on volume or biomass. This means that a single massive tree may be dominant in a plot even if there are many more smaller trees.\n",
        "\n",
        "Yet another approach is calculate dominance based on crown area or basal area. Crown area represents how much of the tree is seen from above (i.e., the crown). Basal area represents the area of the tree stem (i.e., trunk) at breast height.\n",
        "\n",
        "In this tutorial, we will calculate dominance based on total tree volume. This is represented by the `tvol` column in the `trees.csv` dataset.\n",
        "\n",
        "**Question 2 - Fill in the code below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d82d47",
      "metadata": {
        "id": "64d82d47"
      },
      "outputs": [],
      "source": [
        "# Assign a new, unique ID for each tree\n",
        "trees_df['tree_id'] = trees_df.index + 1\n",
        "\n",
        "# Lets examine the distribution of species in the dataset\n",
        "sp_agg = trees_df.groupby('...').agg(\n",
        "    sp_count=('tree_id', 'count')).sort_values(by='sp_count', ascending=False)\n",
        "\n",
        "sp_agg['sp_prop'] = sp_agg['sp_count'] / len(trees_df) * 100\n",
        "\n",
        "sp_agg = round(sp_agg, 2)\n",
        "print(f\"{len(sp_agg)} species found in the dataset.\")\n",
        "print(sp_agg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "# Assign a new, unique ID for each tree\n",
        "trees_df['tree_id'] = trees_df.index + 1\n",
        "\n",
        "# Lets examine the distribution of species in the dataset\n",
        "sp_agg = trees_df.groupby('species').agg(\n",
        "    sp_count=('tree_id', 'count')).sort_values(by='sp_count', ascending=False)\n",
        "\n",
        "sp_agg['sp_prop'] = sp_agg['sp_count'] / len(trees_df) * 100\n",
        "\n",
        "sp_agg = round(sp_agg, 2)\n",
        "print(f\"{len(sp_agg)} species found in the dataset.\")\n",
        "print(sp_agg)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "e9GWfCAtAXaH"
      },
      "id": "e9GWfCAtAXaH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is quite unbalanced with 20/24 species having less than 5% of the total number of trees. This may make for a difficult classification task. Let's group species into a few similar groups to reduce the number of classes."
      ],
      "metadata": {
        "id": "Qez8J9ouKEo6"
      },
      "id": "Qez8J9ouKEo6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aebba62b",
      "metadata": {
        "id": "aebba62b"
      },
      "outputs": [],
      "source": [
        "trees_df['sp_group'] = None\n",
        "\n",
        "#This block of code serches unique species names and assigns them to sp_group\n",
        "for sp in trees_df['species'].unique():\n",
        "\n",
        "    if 'maple' in sp.lower():\n",
        "        trees_df.loc[trees_df['species'] == sp, 'sp_group'] = 'Maple'\n",
        "\n",
        "    elif 'oak' in sp.lower():\n",
        "        trees_df.loc[trees_df['species'] == sp, 'sp_group'] = 'Oak'\n",
        "\n",
        "    elif 'spruce' in sp.lower():\n",
        "        trees_df.loc[trees_df['species'] == sp, 'sp_group'] = 'Spruce'\n",
        "\n",
        "    elif 'ash' in sp.lower():\n",
        "        trees_df.loc[trees_df['species'] == sp, 'sp_group'] = 'Ash'\n",
        "\n",
        "    elif 'birch' in sp.lower():\n",
        "        trees_df.loc[trees_df['species'] == sp, 'sp_group'] = 'Birch'\n",
        "\n",
        "    elif 'aspen' in sp.lower():\n",
        "        trees_df.loc[trees_df['species'] == sp, 'sp_group'] = 'Aspen'\n",
        "\n",
        "    else:\n",
        "        trees_df.loc[trees_df['species'] == sp, 'sp_group'] = sp\n",
        "\n",
        "# Calculates and prints the percentage of each species group in the trees_df\n",
        "print(round(trees_df['sp_group'].value_counts() / len(trees_df) * 100, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can calculate tree dominance based on volume. We can design a helper function for this to assist us.\n",
        "\n",
        "**Question 3 - Please fill in the code below**"
      ],
      "metadata": {
        "id": "DqZhJqX-Pe4V"
      },
      "id": "DqZhJqX-Pe4V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc52ccfd",
      "metadata": {
        "id": "fc52ccfd"
      },
      "outputs": [],
      "source": [
        "def calc_sp_dominance(trees, group_col='sp_group'):\n",
        "\n",
        "    \"\"\"\n",
        "    Calculates the dominant species group within a given DataFrame of trees.\n",
        "\n",
        "    Args:\n",
        "        trees (pd.DataFrame): A DataFrame containing tree data, expected to have\n",
        "                              at least 'tvol' (total volume) and a 'group_col' (species group) column.\n",
        "        group_col (str, optional): The name of the column containing species group information.\n",
        "                                   Defaults to 'sp_group'.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The name of the dominant species group, or None if the input\n",
        "                     DataFrame is empty for the current plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Aggregate the tree data by the specified group_col\n",
        "    trees_agg = trees.groupby(group_col).agg(sp_tvol=('...', 'sum'),)\n",
        "\n",
        "    # Calculate the proportional volume for each species group\n",
        "    trees_agg['sp_vol_prop'] = trees_agg['sp_tvol'] / trees['...'].sum()\n",
        "\n",
        "    # Sort in descending order by total volume\n",
        "    trees_agg = trees_agg.sort_values(by='...', ascending=False).reset_index()\n",
        "\n",
        "    # Get the first row, which holds the dominant species group\n",
        "    if len(trees_agg) == 0:\n",
        "        print(f\"No trees in plot: {trees['PlotName'].iloc[...]}\")\n",
        "        dom_sp_group = None\n",
        "    else:\n",
        "        dom_sp_group = trees_agg[group_col].iloc[0]\n",
        "\n",
        "    return(dom_sp_group)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "def calc_sp_dominance(trees, group_col='sp_group'):\n",
        "\n",
        "    # Aggregate the tree data by the specified group_col\n",
        "    trees_agg = trees.groupby(group_col).agg(sp_tvol=('tvol', 'sum'),)\n",
        "\n",
        "    # Calculate the proportional volume for each species group\n",
        "    trees_agg['sp_vol_prop'] = trees_agg['sp_tvol'] / trees['tvol'].sum()\n",
        "\n",
        "    # Sort in descending order by total volume\n",
        "    trees_agg = trees_agg.sort_values(by='sp_tvol', ascending=False).reset_index()\n",
        "\n",
        "    # Get the first row, which holds the dominant species group\n",
        "    if len(trees_agg) == 0:\n",
        "        print(f\"No trees in plot: {trees['PlotName'].iloc[0]}\")\n",
        "        dom_sp_group = None\n",
        "    else:\n",
        "        dom_sp_group = trees_agg[group_col].iloc[0]\n",
        "\n",
        "    return(dom_sp_group)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "k_mlIK5nCOT7"
      },
      "id": "k_mlIK5nCOT7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can Iterate through each unique plot name in the 'PlotName' column of the DataFrame using our helper function.\n",
        "\n",
        "**Question 4 - fill in the code below.**"
      ],
      "metadata": {
        "id": "z_kJTQ3GB_iX"
      },
      "id": "z_kJTQ3GB_iX"
    },
    {
      "cell_type": "code",
      "source": [
        "for plot_nm in df['...']:\n",
        "\n",
        "    # Filter the 'trees_df' DataFrame to get only the trees belonging to the current plot ('plot_nm').\n",
        "    trees_i = trees_df[trees_df['PlotName'] == plot_nm]\n",
        "\n",
        "    # Call the 'calc_sp_dominance' function for the current plot's trees.\n",
        "    dom_sp_group = ...(trees_i, group_col='sp_group')\n",
        "\n",
        "    # Assign the determined dominant species group ('dom_sp_group') back to the\n",
        "    # 'dom_sp_group' column in the main 'df' DataFrame, for the current plot.\n",
        "    df.loc[df['PlotName'] == plot_nm, 'dom_sp_group'] = dom_sp_group"
      ],
      "metadata": {
        "id": "roHIowH5B5kf"
      },
      "id": "roHIowH5B5kf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "for plot_nm in df['PlotName']:\n",
        "\n",
        "    # Filter the 'trees_df' DataFrame to get only the trees belonging to the current plot ('plot_nm').\n",
        "    trees_i = trees_df[trees_df['PlotName'] == plot_nm]\n",
        "\n",
        "    # Call the 'calc_sp_dominance' function for the current plot's trees.\n",
        "    dom_sp_group = calc_sp_dominance(trees_i, group_col='sp_group')\n",
        "\n",
        "    # Assign the determined dominant species group ('dom_sp_group') back to the\n",
        "    # 'dom_sp_group' column in the main 'df' DataFrame, for the current plot.\n",
        "    df.loc[df['PlotName'] == plot_nm, 'dom_sp_group'] = dom_sp_group"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s1zymrrXHoLH"
      },
      "id": "s1zymrrXHoLH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now summarize `sp_group` proportions in the dataset"
      ],
      "metadata": {
        "id": "ip8OaYbMPuYv"
      },
      "id": "ip8OaYbMPuYv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88a0e291",
      "metadata": {
        "id": "88a0e291"
      },
      "outputs": [],
      "source": [
        "# Calculate the frequency of each dominant species group\n",
        "sp_group_agg = df['dom_sp_group'].value_counts().reset_index()\n",
        "# Calculate the percentage of each dominant species group.\n",
        "sp_group_agg['perc'] = round(sp_group_agg['count'] / len(df) * 100, 2)\n",
        "sp_group_agg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets remove plots dominated by unknown species and Remove plots with species dominant in less than 1% of the dataset."
      ],
      "metadata": {
        "id": "38188yQcQo1i"
      },
      "id": "38188yQcQo1i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b06074d4",
      "metadata": {
        "id": "b06074d4"
      },
      "outputs": [],
      "source": [
        "# Filter out rows where 'dom_sp_group' is 'Unknown'.\n",
        "df = df[df['dom_sp_group'] != 'Unknown']\n",
        "\n",
        "# Identify \"rare\" dominant species groups and filter out plots whose dominant species group is in the 'rare_sp' list.\n",
        "rare_sp = sp_group_agg.loc[sp_group_agg['perc'] < 1, 'dom_sp_group'].tolist()\n",
        "df = df[~df['dom_sp_group'].isin(rare_sp)]\n",
        "\n",
        "# Print the percentage distribution of the remaining dominant species groups.\n",
        "print(df['dom_sp_group'].value_counts()/ len(df) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign nummieric IDs to each dominant species. We can do this by creating a dictionary mapping species names to their IDs.\n",
        "\n",
        "**Question 5  - fill in the code below.**"
      ],
      "metadata": {
        "id": "QgC-_2c4Qymo"
      },
      "id": "QgC-_2c4Qymo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d6aa0e",
      "metadata": {
        "id": "83d6aa0e"
      },
      "outputs": [],
      "source": [
        "# Convert the 'dom_sp_group' column to a categorical data type.\n",
        "df['dom_sp_group_id'] = df['dom_sp_group'].astype('category').cat.codes\n",
        "\n",
        "# Create a dictionary to map the numerical 'dom_sp_group_id' back to their original 'dom_sp_group' names.\n",
        "dom_sp_dict = (df[['...', '...']]\n",
        "               .drop_duplicates()\n",
        "               .sort_values(by='dom_sp_group_id')\n",
        "               .set_index('dom_sp_group_id', drop=True)\n",
        "               .to_dict()['dom_sp_group'])\n",
        "\n",
        "dom_sp_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "# Convert the 'dom_sp_group' column to a categorical data type.\n",
        "df['dom_sp_group_id'] = df['dom_sp_group'].astype('category').cat.codes\n",
        "\n",
        "# Create a dictionary to map the numerical 'dom_sp_group_id' back to their original 'dom_sp_group' names.\n",
        "dom_sp_dict = (df[['dom_sp_group', 'dom_sp_group_id']]\n",
        "               .drop_duplicates()\n",
        "               .sort_values(by='dom_sp_group_id')\n",
        "               .set_index('dom_sp_group_id', drop=True)\n",
        "               .to_dict()['dom_sp_group'])\n",
        "\n",
        "dom_sp_dict"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dN25z5VpIdZO"
      },
      "id": "dN25z5VpIdZO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have aquired all the needed peices to preform our experiment, lets begin!"
      ],
      "metadata": {
        "id": "u36FUzvR66yX"
      },
      "id": "u36FUzvR66yX"
    },
    {
      "cell_type": "markdown",
      "id": "198fd6c6",
      "metadata": {
        "id": "198fd6c6"
      },
      "source": [
        "# Goal 1 - Classify Dominant Species"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets train a random forest model to predict dominant species based on predictors. First, let split the dataset into training and test sets using stratified sampling to preserve the class distribution, ensuring that all species groups are proportionally represented. Then visualize the distribution of classes in both sets to verify balance. A Random Forest model is trained on the selected feature columns `features_ls` and used to predict the species group in the test set. The model's performance is evaluated using accuracy and Cohen's kappa, with the latter accounting for chance agreement. Finally, a confusion matrix heatmap is plotted to show how well the model distinguishes between different species groups, helping to identify where misclassifications occur."
      ],
      "metadata": {
        "id": "SVsZxUZPA8t3"
      },
      "id": "SVsZxUZPA8t3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code chunk below, we break the random forest modelling into three general steps:\n",
        "\n",
        "1) divide the dataset into training and testing using a 70% - 30% split.\n",
        "\n",
        "2) Train the random forest variable using the defined explanatory variables (i.e., features) and response variable.\n",
        "\n",
        "3) Evaluate the random forest classifier using a confusion matrix and metrics (overall accuracy and kappa).\n"
      ],
      "metadata": {
        "id": "v0JdZ9nvQCgo"
      },
      "id": "v0JdZ9nvQCgo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc5c61b6",
      "metadata": {
        "id": "fc5c61b6"
      },
      "outputs": [],
      "source": [
        "# 1) Use simple train/test split using stratified sampling for now, apply cross-validation later\n",
        "train_df, test_df =  train_test_split(df[['dom_sp_group_id'] + features_ls],\n",
        "                 stratify=df['dom_sp_group_id'],\n",
        "                 test_size=0.3,\n",
        "                 random_state=25)\n",
        "\n",
        "\n",
        "# 2) Train the random forest variable using the defined explanatory variables (i.e., features) and response variable.\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=25)\n",
        "rf.fit(train_df[features_ls], train_df['dom_sp_group_id'])\n",
        "\n",
        "# Apply the model to the test set\n",
        "test_df['pred_dom_sp_group_id'] = rf.predict(test_df[features_ls])\n",
        "\n",
        "# 3) Calculate accuracy and kappa\n",
        "accuracy = accuracy_score(test_df['dom_sp_group_id'], test_df['pred_dom_sp_group_id'])\n",
        "kappa = cohen_kappa_score(test_df['dom_sp_group_id'], test_df['pred_dom_sp_group_id'])\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Kappa: {kappa:.2f}\")\n",
        "\n",
        "# Compare the distribution of dominant species in the training and test sets and print confusion matrix side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "sns.histplot(train_df['dom_sp_group_id'], ax=axes[0], color='skyblue', label='Train', kde=False)\n",
        "sns.histplot(test_df['dom_sp_group_id'], ax=axes[0], color='salmon', label='Test', kde=False)\n",
        "axes[0].set_title('Train vs Test Set Distribution')\n",
        "axes[0].legend()\n",
        "\n",
        "# Print confusion matrix\n",
        "conf_matrix = confusion_matrix(test_df['dom_sp_group_id'], test_df['pred_dom_sp_group_id'])\n",
        "\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=dom_sp_dict.values(),\n",
        "            yticklabels=dom_sp_dict.values(),\n",
        "            ax=axes[1])\n",
        "axes[1].set_title('Confusion Matrix')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 - After examining the cell output, what can be said about our classification model?**"
      ],
      "metadata": {
        "id": "-yFZeOs7EXLY"
      },
      "id": "-yFZeOs7EXLY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer here*"
      ],
      "metadata": {
        "id": "eaxIhas0al31"
      },
      "id": "eaxIhas0al31"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details open>\n",
        "<summary>Solution</summary>\n",
        "As can be seen in the confusion matrix and classification metrics above, the model performance is not great. It is possible this could be improved somewhat by tuning hyperparameters, curating the datasets, etc.\n",
        "In this case, we will further simplify the problem to improve our results. Now instead of classifying tree species group, we will simply perform a  classification to determine whether a forest is coniferous (evergreen) vs. deciduous (leafy) dominated, or is mixed.\n",
        "</details>"
      ],
      "metadata": {
        "id": "ogIjUlYuQQ_4"
      },
      "id": "ogIjUlYuQQ_4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that our model is not accurate, we will try to reprocess the data to be more useful to decision tree models."
      ],
      "metadata": {
        "id": "VS_-dLHDB07V"
      },
      "id": "VS_-dLHDB07V"
    },
    {
      "cell_type": "markdown",
      "id": "6ef85814",
      "metadata": {
        "id": "6ef85814"
      },
      "source": [
        "### Classify Dominant Tree Type (Coniferous/Deciduous)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For optimal performance, decision trees rely on binary decisions. This means each question within the tree should have only two possible answers, like \"yes/no\" or \"greater than/less than,\" requiring your input to be structured accordingly."
      ],
      "metadata": {
        "id": "SRObUnv0D8XI"
      },
      "id": "SRObUnv0D8XI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group the species in the dataset based on coniferous/deciduous. To determine whether a plot is coniferous/deciduous dominated, we will use a volume threshold of 75%. In other words, if >75% of the plot tree volume is coniferous, it is considered to be a coniferous stand, with the same logic applied to deciduous. If a plot's volume is <75% dominated, it is considered mixed. You can adjust this threshold to see how it affects model performance.\n",
        "\n",
        "Let's simplify the problem and just classify whether a plot is broadleaf or deciduous. Create a dict indicating whether a species is coniferous ('c') or deciduous ('d')\n",
        "\n",
        "> **_Note - Why do we do this:_** Before we can label a plot (which contains many trees), we need to know what type each species is. This dictionary maps each species name to 'c' (coniferous) or 'd' (deciduous).\n",
        "\n",
        "**Question 2 - Please fill in the code below.**"
      ],
      "metadata": {
        "id": "d3t1CgitSmhE"
      },
      "id": "d3t1CgitSmhE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e6fd8d",
      "metadata": {
        "id": "74e6fd8d"
      },
      "outputs": [],
      "source": [
        "dom_sp_type_dict = {\n",
        "    'Balsam fir': 'c',\n",
        "    'White pine': 'c',\n",
        "    'Red (soft) maple': 'd',\n",
        "    'Red pine': 'c',\n",
        "    'Jack pine': 'c',\n",
        "    'Red oak': 'd',\n",
        "    'White spruce': 'c',\n",
        "    'Tamarack': 'c',\n",
        "    'Sugar Maple': 'd',\n",
        "    'Trembling Aspen': 'd',\n",
        "    'Ironwood': 'd',\n",
        "    'Norway Spruce': 'c',\n",
        "    'American beech': 'd',\n",
        "    'Black ash': 'd',\n",
        "    'White birch': 'd',\n",
        "    'Largetooth aspen': 'd',\n",
        "    'Yellow birch': 'd',\n",
        "    'Basswood': 'd',\n",
        "    'Northern white cedar': 'c',\n",
        "    'Eastern hemlock': 'c',\n",
        "    'White ash': 'd',\n",
        "    'American elm': 'd',\n",
        "    'Black cherry': 'd',\n",
        "    'Balsam poplar': 'd'\n",
        "}\n",
        "\n",
        "# Convert to df\n",
        "sp_type_df = (pd.DataFrame.from_dict(dom_sp_type_dict, columns=['dom_sp_type'], orient='index')\n",
        "              .reset_index()\n",
        "              .rename(columns={'index': 'species'}))\n",
        "\n",
        "# Join with trees\n",
        "trees_df = trees_df.merge(..., on='species', how='left')\n",
        "\n",
        "trees_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "dom_sp_type_dict = {\n",
        "    'Balsam fir': 'c',\n",
        "    'White pine': 'c',\n",
        "    'Red (soft) maple': 'd',\n",
        "    'Red pine': 'c',\n",
        "    'Jack pine': 'c',\n",
        "    'Red oak': 'd',\n",
        "    'White spruce': 'c',\n",
        "    'Tamarack': 'c',\n",
        "    'Sugar Maple': 'd',\n",
        "    'Trembling Aspen': 'd',\n",
        "    'Ironwood': 'd',\n",
        "    'Norway Spruce': 'c',\n",
        "    'American beech': 'd',\n",
        "    'Black ash': 'd',\n",
        "    'White birch': 'd',\n",
        "    'Largetooth aspen': 'd',\n",
        "    'Yellow birch': 'd',\n",
        "    'Basswood': 'd',\n",
        "    'Northern white cedar': 'c',\n",
        "    'Eastern hemlock': 'c',\n",
        "    'White ash': 'd',\n",
        "    'American elm': 'd',\n",
        "    'Black cherry': 'd',\n",
        "    'Balsam poplar': 'd'\n",
        "}\n",
        "\n",
        "# Convert to df\n",
        "sp_type_df = (pd.DataFrame.from_dict(dom_sp_type_dict, columns=['dom_sp_type'], orient='index')\n",
        "              .reset_index()\n",
        "              .rename(columns={'index': 'species'}))\n",
        "\n",
        "# Join with trees\n",
        "trees_df = trees_df.merge(sp_type_df, on='species', how='left')\n",
        "\n",
        "trees_df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "opRM-EMMM72a"
      },
      "id": "opRM-EMMM72a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will classify a stand as Conifer (c) or Deciduous (d) dominated if the volume of a given type is greater than a specified threshold percentage\n",
        "\n",
        "> **_Note - Why do we do this:_** We define a dominance threshold. If ≥75% of a plot’s tree volume is coniferous, it’s a coniferous plot. If ≤25% is coniferous, it’s deciduous. Anything in between is considered mixed. This simplification reduces a complex problem (many species and combinations) into a binary or 3-class classification problem, which is easier for a model to handle."
      ],
      "metadata": {
        "id": "MtYIpm5DTFKm"
      },
      "id": "MtYIpm5DTFKm"
    },
    {
      "cell_type": "code",
      "source": [
        "CONIF_PERC_THRESH = 75"
      ],
      "metadata": {
        "id": "-pfUNHjAFQrz"
      },
      "id": "-pfUNHjAFQrz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code below calculates the percentage of conifer volume for each plot and classifies plots as coniferous ('c'), deciduous ('d'), or mixed ('m') based on a conifer percentage threshold. First, it defines a function calc_conifer_perc() to compute the percentage of total tree volume made up of conifers (identified by dom_sp_type == 'c'). For each plot in the dataset, it calculates this percentage using tree-level data (trees_df) and stores the result in a new column conif_perc in the plot-level dataframe (df).\n",
        "\n",
        "\n",
        "> **_Note - Why do we do this:_** We aggregate tree volumes within each plot to calculate the percentage of volume made up by coniferous trees. This derived feature (conif_perc) is used to classify the plot’s dominant tree type.\n",
        "\n",
        "**Question 3 - Please fill in the code below.**\n"
      ],
      "metadata": {
        "id": "2H4NLjiDGL2J"
      },
      "id": "2H4NLjiDGL2J"
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_conifer_perc(trees):\n",
        "\n",
        "    conif_df = trees[trees['...'] == '...'].reset_index(drop=True)\n",
        "\n",
        "    total_vol = trees['...'].sum()\n",
        "\n",
        "    conif_vol = conif_df['...'].sum()\n",
        "\n",
        "    conif_perc = round(conif_vol / total_vol * 100, 2)\n",
        "\n",
        "    return(conif_perc)\n",
        "\n",
        "for plot_nm in df['...']:\n",
        "\n",
        "    trees_i = trees_df[trees_df['PlotName'] == plot_nm]\n",
        "\n",
        "    conif_perc = calc_conifer_perc(trees_i)\n",
        "\n",
        "    df.loc[df['PlotName'] == plot_nm, 'conif_perc'] = conif_perc"
      ],
      "metadata": {
        "id": "aJIlrQ3VB05o"
      },
      "id": "aJIlrQ3VB05o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "def calc_conifer_perc(trees):\n",
        "\n",
        "    conif_df = trees[trees['dom_sp_type'] == 'c'].reset_index(drop=True)\n",
        "\n",
        "    total_vol = trees['tvol'].sum()\n",
        "\n",
        "    conif_vol = conif_df['tvol'].sum()\n",
        "\n",
        "    conif_perc = round(conif_vol / total_vol * 100, 2)\n",
        "\n",
        "    return(conif_perc)\n",
        "\n",
        "for plot_nm in df['PlotName']:\n",
        "\n",
        "    trees_i = trees_df[trees_df['PlotName'] == plot_nm]\n",
        "\n",
        "    conif_perc = calc_conifer_perc(trees_i)\n",
        "\n",
        "    df.loc[df['PlotName'] == plot_nm, 'conif_perc'] = conif_perc"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sqhcJGNHQS_3"
      },
      "id": "sqhcJGNHQS_3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the classify_plot_type() function. Each plot is labeled as coniferous if its percentage is above the threshold, deciduous if below the complement of the threshold (i.e. ≤25%), or mixed otherwise. These classifications are converted into an ordered categorical type to support proper sorting or modeling.\n",
        "\n",
        "> **_Note - Why do we do this:_** We label each plot based on its dominant tree type. Categorizing this is essential for visualizing data distribution and defining the target variable (plot_type) for training the classifier."
      ],
      "metadata": {
        "id": "geNS71bACEPN"
      },
      "id": "geNS71bACEPN"
    },
    {
      "cell_type": "code",
      "source": [
        "# Classify plots as coniferous (c), deciduous (d), or mixed (m) based on the conifer percentage\n",
        "def classify_plot_type(conif_perc, conif_perc_thresh):\n",
        "\n",
        "    if (conif_perc >= conif_perc_thresh):\n",
        "        return 'c'\n",
        "\n",
        "    elif conif_perc <= abs(conif_perc_thresh - 100):\n",
        "        return 'd'\n",
        "    else:\n",
        "        return 'm'"
      ],
      "metadata": {
        "id": "WzTvfM8dB5h-"
      },
      "id": "WzTvfM8dB5h-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the code visualizes the distribution of these plot types and prints the percentage of each class across the dataset.\n",
        "\n",
        "> **_Note - Why do we do this:_** Before modeling, we need to understand class balance (e.g., are most plots coniferous?). This affects model performance and determine need for class weighting or resampling"
      ],
      "metadata": {
        "id": "RvEZWJZNCJ2m"
      },
      "id": "RvEZWJZNCJ2m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4edc70d",
      "metadata": {
        "id": "a4edc70d"
      },
      "outputs": [],
      "source": [
        "df['dom_type'] = df['conif_perc'].apply(lambda x: classify_plot_type(x, CONIF_PERC_THRESH))\n",
        "\n",
        "# Convert to ordered catopegory\n",
        "cat_type = CategoricalDtype(categories=['c', 'd', 'm'], ordered=True)\n",
        "\n",
        "df['dom_type'] = df['dom_type'].astype(cat_type)\n",
        "\n",
        "# View distribution of dominant types\n",
        "df['dom_type'].hist()\n",
        "\n",
        "print(round(df['dom_type'].value_counts() / len(df) * 100), 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the data from above, we can train a new random forest model to predict broadleaf vs coniferous.\n",
        "\n",
        "**Question 4 - Fill in the code below.**\n"
      ],
      "metadata": {
        "id": "9GRDwz2aGTDW"
      },
      "id": "9GRDwz2aGTDW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab9a6e51",
      "metadata": {
        "id": "ab9a6e51"
      },
      "outputs": [],
      "source": [
        "train_df, test_df =  ...(df[['dom_type'] + features_ls],\n",
        "                 stratify=df['dom_type'],\n",
        "                 test_size=0.3,\n",
        "                 random_state=25)\n",
        "\n",
        "rf = ...(n_estimators=100, random_state=25)\n",
        "\n",
        "rf....(train_df[features_ls], train_df['dom_type'])\n",
        "\n",
        "# Apply the model to the test set\n",
        "test_df['pred_dom_type'] = rf....(test_df[features_ls])\n",
        "\n",
        "# Calculate accuracy and kappa\n",
        "accuracy = accuracy_score(test_df['dom_type'], test_df['pred_dom_type'])\n",
        "kappa = cohen_kappa_score(test_df['dom_type'], test_df['pred_dom_type'])\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Kappa: {kappa:.2f}\")\n",
        "\n",
        "# Print confusion matrix\n",
        "\n",
        "conf_matrix = ...(test_df['dom_type'], test_df['pred_dom_type'])\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Coniferous', 'Deciduous', 'Mixed'],\n",
        "            yticklabels=['Coniferous', 'Deciduous', 'Mixed'])\n",
        "\n",
        "print(test_df['dom_type'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "train_df, test_df =  train_test_split(df[['dom_type'] + features_ls],\n",
        "                 stratify=df['dom_type'],\n",
        "                 test_size=0.3,\n",
        "                 random_state=25)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=25)\n",
        "\n",
        "rf.fit(train_df[features_ls], train_df['dom_type'])\n",
        "\n",
        "# Apply the model to the test set\n",
        "test_df['pred_dom_type'] = rf.predict(test_df[features_ls])\n",
        "\n",
        "# Calculate accuracy and kappa\n",
        "accuracy = accuracy_score(test_df['dom_type'], test_df['pred_dom_type'])\n",
        "kappa = cohen_kappa_score(test_df['dom_type'], test_df['pred_dom_type'])\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Kappa: {kappa:.2f}\")\n",
        "\n",
        "# Print confusion matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(test_df['dom_type'], test_df['pred_dom_type'])\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Coniferous', 'Deciduous', 'Mixed'],\n",
        "            yticklabels=['Coniferous', 'Deciduous', 'Mixed'])\n",
        "\n",
        "print(test_df['dom_type'].value_counts())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "01w0ycxARA1O"
      },
      "id": "01w0ycxARA1O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much better accuracy!"
      ],
      "metadata": {
        "id": "5OYTT_UMOhLu"
      },
      "id": "5OYTT_UMOhLu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5 - Investigate qualities of plots that were misclassified. Why do you think these plots were misclassified?**"
      ],
      "metadata": {
        "id": "-GoclW0gHpma"
      },
      "id": "-GoclW0gHpma"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer here*"
      ],
      "metadata": {
        "id": "vjJuf54JcpKz"
      },
      "id": "vjJuf54JcpKz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details open>\n",
        "<summary>Solution</summary>\n",
        "General Reasons for Misclassification:\n",
        "\n",
        "- Feature Limitations: The features extracted from the plots (e.g., spectral bands, texture, height) may not perfectly capture the distinct characteristics that differentiate these forest types, especially in transitional or heterogeneous areas.\n",
        "\n",
        "- Training Data Bias/Quality: If the training data for the model was not perfectly representative of all variations within each class, or if there were labeling errors in the training data, it could lead to misclassifications.\n",
        "\n",
        "- Boundary Ambiguity: The definitions of \"Coniferous,\" \"Deciduous,\" and \"Mixed\" can sometimes have fuzzy boundaries in nature. A plot might genuinely exhibit characteristics that fall between categories.\n",
        "\n",
        "- Environmental Factors: Variations in light conditions, atmospheric effects (if using remote sensing), or ground conditions can influence the data and lead to misinterpretations.\n",
        "\n",
        "- Model Complexity/Hyperparameters: The model itself (e.g., its algorithm, hyperparameters) might not be optimally tuned to handle the nuances of these classifications.\n",
        "\n",
        "- Spatial Resolution: If using imagery, the resolution might not be fine enough to distinguish individual trees or small patches of different species within a plot.\n",
        "</details>"
      ],
      "metadata": {
        "id": "yQXbGVCqSMH3"
      },
      "id": "yQXbGVCqSMH3"
    },
    {
      "cell_type": "markdown",
      "id": "16a7ce3d",
      "metadata": {
        "id": "16a7ce3d"
      },
      "source": [
        "# Goal 2 - Regress Conifer Percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifying a plot as coniferous, deciduous or mixed is simple and effective, however it not capture the nuance of species composition. An interesting approach to machine learning is considering how different modelling objectives can be approached as either classification, or regression problems.\n",
        "\n",
        "Below we will apply a random forest regressor to estimate coniferous proportion to derive more specific information about species composition in the dataset."
      ],
      "metadata": {
        "id": "WIXAiXupJG2e"
      },
      "id": "WIXAiXupJG2e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First lets view the conifer percentage to get an idea of the data."
      ],
      "metadata": {
        "id": "2nUiigkcUlfr"
      },
      "id": "2nUiigkcUlfr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50eadbb0",
      "metadata": {
        "id": "50eadbb0"
      },
      "outputs": [],
      "source": [
        "df['conif_perc'].hist()\n",
        "plt.xlabel('conif_perc')\n",
        "plt.ylabel('Frequency')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a random forest model to predict balsam fir percentage\n",
        "\n",
        "**Question 1 - fill in the code below.**"
      ],
      "metadata": {
        "id": "YO3QaxraJPOk"
      },
      "id": "YO3QaxraJPOk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb3efa86",
      "metadata": {
        "id": "cb3efa86"
      },
      "outputs": [],
      "source": [
        "train_df, test_df =  ...(df[['conif_perc'] + features_ls],\n",
        "                                        test_size=0.3,\n",
        "                                        random_state=25)\n",
        "\n",
        "rf = ...(n_estimators=100, random_state=25)\n",
        "\n",
        "rf....(train_df[features_ls], train_df['conif_perc'])\n",
        "\n",
        "# Apply the model to the test set\n",
        "test_df['pred_conif_perc'] = rf....(test_df[features_ls])\n",
        "\n",
        "# Calculate regression metrics\n",
        "\n",
        "rmse = sqrt(mean_squared_error(test_df['...'], test_df['pred_conif_perc']))\n",
        "r2 = r2_score(test_df['...'], test_df['pred_conif_perc'])\n",
        "\n",
        "print(f\"RMSE: {rmse:.5f}\")\n",
        "print(f\"R²: {r2:.5f}\")\n",
        "\n",
        "# Plot the predicted vs actual values\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=test_df['conif_perc'], y=test_df['pred_conif_perc'])\n",
        "plt.xlabel(f'Actual conif_perc Percentage')\n",
        "plt.ylabel(f'Predicted conif_perc Percentage')\n",
        "plt.grid(True)\n",
        "plt.plot([0, 100], [0, 100], color='red', linestyle='--', label='1:1 Line')\n",
        "\n",
        "plt.xlim(0, 100)\n",
        "plt.ylim(0, 100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "train_df, test_df =  train_test_split(df[['conif_perc'] + features_ls],\n",
        "                                        test_size=0.3,\n",
        "                                        random_state=25)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=25)\n",
        "\n",
        "rf.fit(train_df[features_ls], train_df['conif_perc'])\n",
        "\n",
        "# Apply the model to the test set\n",
        "test_df['pred_conif_perc'] = rf.predict(test_df[features_ls])\n",
        "\n",
        "# Calculate regression metrics\n",
        "\n",
        "rmse = sqrt(mean_squared_error(test_df['conif_perc'], test_df['pred_conif_perc']))\n",
        "r2 = r2_score(test_df['conif_perc'], test_df['pred_conif_perc'])\n",
        "\n",
        "print(f\"RMSE: {rmse:.5f}\")\n",
        "print(f\"R²: {r2:.5f}\")\n",
        "\n",
        "# Plot the predicted vs actual values\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=test_df['conif_perc'], y=test_df['pred_conif_perc'])\n",
        "plt.xlabel(f'Actual conif_perc Percentage')\n",
        "plt.ylabel(f'Predicted conif_perc Percentage')\n",
        "plt.grid(True)\n",
        "plt.plot([0, 100], [0, 100], color='red', linestyle='--', label='1:1 Line')\n",
        "\n",
        "plt.xlim(0, 100)\n",
        "plt.ylim(0, 100)"
      ],
      "metadata": {
        "id": "7IdsW3H8Uu9k",
        "cellView": "form"
      },
      "id": "7IdsW3H8Uu9k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpret the RMSE, what are the units? What does this represent?\n",
        "\n",
        "Discuss the difference in performance between classifying dominant tree type (coniferous, deciduous, mixed) versus regressing coniferous prorpotion."
      ],
      "metadata": {
        "id": "gdSIOlvRJUns"
      },
      "id": "gdSIOlvRJUns"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer here*"
      ],
      "metadata": {
        "id": "-xUc0X59fBE4"
      },
      "id": "-xUc0X59fBE4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details open>\n",
        "<summary>Solution</summary>\n",
        "1. Classification (Dominant Tree Type):\n",
        "\n",
        " - Goal: To assign each plot to one of a predefined set of discrete categories (e.g., Coniferous, Deciduous, Mixed).\n",
        "\n",
        "- Strengths (as seen in the confusion matrix):\n",
        "\n",
        "    - Simplicity and Interpretability for Discrete Classes: It provides a clear, single label for a plot, which can be useful for broad management decisions (e.g., \"This entire stand is coniferous\").\n",
        "\n",
        "    - Good for Clear-Cut Distinctions: When there are strong, unambiguous differences between categories (e.g., pure coniferous vs. pure deciduous), classification models can perform very well (as evidenced by 0 misclassifications between Coniferous and Deciduous in the previous matrix).\n",
        "\n",
        "    - Handles Non-Linear Relationships: Classification models can capture complex decision boundaries between classes.\n",
        "\n",
        "- Weaknesses (implied by the data):\n",
        "\n",
        "    - Loss of Nuance (Especially for \"Mixed\"): The \"Mixed\" category in classification lumps together plots with a wide range of coniferous proportions (e.g., 20% coniferous, 80% deciduous vs. 70% coniferous, 30% deciduous). This loses valuable information about the degree of mixing. The misclassifications in the \"Mixed\" category in the previous image highlight this; a truly \"mixed\" plot might be labeled coniferous if it's heavily coniferous-dominated, or deciduous if it's deciduous-dominated, even if it still contains both.\n",
        "\n",
        "     - Arbitrary Thresholds: Classification requires defining thresholds to separate categories (e.g., \"mixed\" means between X% and Y% coniferous). These thresholds can be somewhat arbitrary and lead to misclassifications for plots near the boundaries.\n",
        "\n",
        "     - Hard Boundaries: A classification model forces a hard decision, even when a plot might be borderline between two types.\n",
        "\n",
        "2. Regression (Coniferous Proportion):\n",
        "\n",
        "- Goal: To predict a continuous numerical value (the exact percentage of coniferous trees).\n",
        "\n",
        "- Strengths (as seen in the scatter plot):\n",
        "\n",
        "    - Captures Granular Information: Regression provides a more detailed and quantitative understanding of the forest composition. Instead of just \"mixed,\" it tells you how mixed (e.g., 45% coniferous). This is crucial for precise ecological studies, biomass estimation, or timber volume calculations.\n",
        "\n",
        "    - No Arbitrary Thresholds: The model directly predicts the proportion, avoiding the need for predefined categories.\n",
        "\n",
        "    - Better for Gradual Transitions: It naturally handles the continuum of forest compositions from pure deciduous to pure coniferous, including all degrees of mixing. The red dashed line in the scatter plot (representing perfect prediction) shows that the goal is to align actual with predicted values across the entire range.\n",
        "\n",
        "- Weaknesses (as seen in the scatter plot):\n",
        "\n",
        "    - More Challenging for High Accuracy: Predicting an exact percentage can be more difficult than assigning a broad category. The scatter of points around the dashed line indicates that there is variability in the predictions, especially in the middle ranges (e.g., 20-80% actual coniferous).\n",
        "\n",
        "    - Sensitivity to Outliers: As mentioned, RMSE is sensitive to larger errors. A few highly inaccurate predictions can significantly impact the RMSE.\n",
        "\n",
        "    - Interpretation for \"Dominant Type\": While it gives you the proportion, if you need a single dominant type label, you'd still have to apply thresholds to the regression output (e.g., >70% coniferous = coniferous, <30% = deciduous, else mixed). This brings back some of the issues of classification.\n",
        "</detais>"
      ],
      "metadata": {
        "id": "l2tEIytcVvkI"
      },
      "id": "l2tEIytcVvkI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model shows strong performance, but we've identified areas for enhancement. Our future goals include implementing strategies that boost performance while meticulously avoiding overfitting and data skew."
      ],
      "metadata": {
        "id": "N6Ptjvp0QiWF"
      },
      "id": "N6Ptjvp0QiWF"
    },
    {
      "cell_type": "markdown",
      "id": "af814525",
      "metadata": {
        "id": "af814525"
      },
      "source": [
        "# Goal 3- Add feature selection using a method from SKlearn:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/feature_selection.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection is a technique used to pick the most relevant features (or variables) from your dataset. This helps your machine learning model focus on what's truly important.\n",
        "\n",
        "One key reason to use feature selection is to address highly correlated features. When two or more features in your dataset are strongly correlated, they essentially carry very similar information. If your model is trained with all of these redundant features, it can lead to overfitting.\n",
        "\n",
        "Reasons:\n",
        "\n",
        "- Redundancy: The model tries to learn from information that's already largely represented by another feature, making it unnecessarily complex.\n",
        "\n",
        "- Noise Absorption: An overly complex model, especially with redundant features, might start to \"memorize\" the specific quirks and noise in your training data rather than the underlying patterns.\n",
        "\n",
        "- Reduced Generalization: When a model overfits, it performs really well on the data it was trained on but struggles significantly when it encounters new, unseen data. This means its real-world performance will be poor.\n",
        "\n",
        "By removing these highly correlated features, you're essentially simplifying the model. This makes it less prone to overfitting and helps it generalize better to new data. Ultimately, this often leads to a better performing model that's more robust and reliable."
      ],
      "metadata": {
        "id": "IbJSII4rgckr"
      },
      "id": "IbJSII4rgckr"
    },
    {
      "cell_type": "markdown",
      "id": "347d6e83",
      "metadata": {
        "id": "347d6e83"
      },
      "source": [
        "First we will check Correlation Between Predictors. These often impact scoring."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate correlation matrix\n",
        "corr_matrix = df[features_ls].corr().abs()\n",
        "\n",
        "# Visualize correlation matrix (optional)\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, cmap=\"coolwarm\", linewidths=0.5)\n",
        "plt.title(\"Predictor Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0h6DX4sgLSOx"
      },
      "id": "0h6DX4sgLSOx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us an idea that there does exists a bunch of highly correlated features. There exists methods in sklearn which can help reduce or remove them."
      ],
      "metadata": {
        "id": "YaNSmWTgHHvQ"
      },
      "id": "YaNSmWTgHHvQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "`from sklearn.feature_selection import RFECV`\n",
        "\n",
        "This imports **Recursive Feature Elimination with Cross-Validation (RFECV)** from `scikit-learn`.\n",
        "\n",
        "- Selects the most important features by:\n",
        "  1. Fitting a model using all features.\n",
        "  2. Ranking feature importance.\n",
        "  3. Removing the least important feature(s) recursively.\n",
        "  4. Using **cross-validation** to find the optimal number of features.\n",
        "\n",
        "- Helps reduce overfitting.\n",
        "- Speeds up training by removing irrelevant features.\n",
        "- Improves model interpretability.\n",
        "\n",
        "\n",
        "`from sklearn.preprocessing import StandardScaler`\n",
        "\n",
        "This imports the StandardScaler from scikit-learn.\n",
        "\n",
        "\n",
        "- Standardizes the features by:\n",
        "\n",
        "    1. Removing the mean (centering).\n",
        "    2. Scaling to unit variance.\n",
        "    3. After scaling, each feature will have:\n",
        "    4. Mean = 0\n",
        "    5. Standard deviation = 1\n",
        "\n",
        "- Required or beneficial for many ML algorithms like:\n",
        "    1. Logistic Regression\n",
        "    2. SVMs\n",
        "    3. K-Nearest Neighbors (KNN)\n",
        "    4. PCA\n",
        "    5. Ensures that features are on the same scale."
      ],
      "metadata": {
        "id": "CgCukMqQWEB3"
      },
      "id": "CgCukMqQWEB3"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "-658iaFlQXPo"
      },
      "id": "-658iaFlQXPo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training the regression model, start by making a copy of the original DataFrame to avoid modifying the raw data. Next, define the feature matrix X using the selected feature list (features_ls) and set the target variable y as the column representing conifer percentage (conif_perc). Since many machine learning models perform better with standardized input, apply feature scaling using StandardScaler to transform the features so they have a mean of 0 and standard deviation of 1. Finally, convert the scaled array back into a DataFrame with the same column names and index as the original.\n",
        "\n",
        "**Question 1 - Please fill in the code below.**"
      ],
      "metadata": {
        "id": "NQj9VbzoeXPn"
      },
      "id": "NQj9VbzoeXPn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy of the original dataframe\n",
        "df_fs = df.copy()\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df_fs[...]\n",
        "y = df_fs['...']\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=features_ls, index=X.index)"
      ],
      "metadata": {
        "id": "vWCgnpkAQSuE"
      },
      "id": "vWCgnpkAQSuE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "# Make a copy of the original dataframe\n",
        "df_fs = df.copy()\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df_fs[features_ls]\n",
        "y = df_fs['conif_perc']\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=features_ls, index=X.index)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9UuFX-gWWy6S"
      },
      "id": "9UuFX-gWWy6S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After preprocessing the data, split the scaled dataset into training and testing sets to evaluate model performance on unseen data. In this case, 30% of the data is reserved for testing, and a fixed random seed ensures reproducibility. Then, initialize a RandomForestRegressor model with 100 estimators and a fixed random state for consistent results. To perform feature selection, use RFECV (Recursive Feature Elimination with Cross-Validation), which repeatedly removes the least important features while evaluating model performance using 5-fold cross-validation. It uses negative mean squared error as the scoring metric. Finally, fit the RFECV object to the training data to identify the optimal subset of features.\n",
        "\n",
        "**Question 2 - Please fill in the code below.**\n",
        "\n",
        "\n",
        "> **_Note:_** This code might take some time to finish running. It should take no more than 5 minutes."
      ],
      "metadata": {
        "id": "3rnOtEXee9Rq"
      },
      "id": "3rnOtEXee9Rq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the scaled data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=25)\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "estimator = ...(n_estimators=100, random_state=25)\n",
        "\n",
        "# Initialize RFECV\n",
        "rfecv = ...(estimator=estimator, step=1, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "\n",
        "# Fit RFECV to find the optimal number of features\n",
        "rfecv.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "aV-ubQgrXaTr"
      },
      "id": "aV-ubQgrXaTr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "# Split the scaled data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=25)\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "estimator = RandomForestRegressor(n_estimators=100, random_state=25)\n",
        "\n",
        "# Initialize RFECV\n",
        "rfecv = RFECV(estimator=estimator, step=1, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "\n",
        "# Fit RFECV to find the optimal number of features\n",
        "rfecv.fit(X_train, y_train)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "idUNwnhyXS3y"
      },
      "id": "idUNwnhyXS3y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the code below will show some stats about the Random forest model."
      ],
      "metadata": {
        "id": "OL7jvMXrfigO"
      },
      "id": "OL7jvMXrfigO"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Optimal number of features : {rfecv.n_features_}\")\n",
        "print(\"Selected features (True means selected):\")\n",
        "print(rfecv.support_)"
      ],
      "metadata": {
        "id": "knAs0re-XjvF"
      },
      "id": "knAs0re-XjvF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once RFECV has identified the most important features, extract their names by using the support_ mask from the fitted RFECV object and apply it to the training DataFrame’s column names. This gives a list of features that were selected as optimal for the model. To better understand how the number of features affects model performance, visualize the cross-validation results. We created a line plot showing the number of features selected on the x-axis and the corresponding negative mean squared error on the y-axis. This plot helps illustrate how model performance changes as more or fewer features are included in training."
      ],
      "metadata": {
        "id": "RK3rxSjtfIFR"
      },
      "id": "RK3rxSjtfIFR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the names of the selected features\n",
        "selected_features = X_train.columns[rfecv.support_].tolist()\n",
        "print(f\"\\nSelected features: {selected_features}\")\n",
        "\n",
        "# Visualize the RFE scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title('Recursive Feature Elimination with Cross-Validation')\n",
        "plt.xlabel('Number of features selected')\n",
        "plt.ylabel('Negative Mean Squared Error (Higher is better)')\n",
        "\n",
        "# Corrected line: Use .cv_results_['mean_test_score'] from the estimator within RFECV\n",
        "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8JMIEZpJXqSp"
      },
      "id": "8JMIEZpJXqSp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the graph indicates, performance plateaus around 23 features. Selecting additional features would lead to overfitting and is unnecessary."
      ],
      "metadata": {
        "id": "k66xhJfOhmUc"
      },
      "id": "k66xhJfOhmUc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "After selecting the optimal subset of features using RFECV, retrain the RandomForestRegressor using only those selected features. Fit the model on the training data and then make predictions on the test set using the same selected features. To evaluate how well the model performs, calculate regression metrics including Root Mean Squared Error (RMSE) and the coefficient of determination (R²). These metrics give insight into prediction accuracy and how well the model explains the variance in the target variable. Finally, create a scatter plot comparing the actual versus predicted values for the conifer percentage. The plot includes a red dashed line representing a perfect 1:1 prediction. The closer the points are to this line, the better the model's predictive performance.\n",
        "\n",
        "**Question 3 - fill in the code below.**"
      ],
      "metadata": {
        "id": "Psgmqo-ffPcS"
      },
      "id": "Psgmqo-ffPcS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Random Forest Regressor with only the selected features\n",
        "rf_reg_selected = ...(n_estimators=100, random_state=25)\n",
        "rf_reg_selected.fit(X_train[selected_features], y_train)\n",
        "\n",
        "# Make predictions on the test set using only the selected features\n",
        "y_pred_selected = rf_reg_selected....(X_test[selected_features])\n",
        "\n",
        "# Calculate and print regression metrics for the model with selected features\n",
        "rmse_selected = sqrt(mean_squared_error(y_test, y_pred_selected))\n",
        "r2_selected = r2_score(y_test, y_pred_selected)\n",
        "\n",
        "print(f\"\\nRMSE with selected features: {rmse_selected:.5f}\")\n",
        "print(f\"R² with selected features: {r2_selected:.5f}\")"
      ],
      "metadata": {
        "id": "6DoJIB76XjVa"
      },
      "id": "6DoJIB76XjVa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "# Train the Random Forest Regressor with only the selected features\n",
        "rf_reg_selected = RandomForestRegressor(n_estimators=100, random_state=25)\n",
        "rf_reg_selected.fit(X_train[selected_features], y_train)\n",
        "\n",
        "# Make predictions on the test set using only the selected features\n",
        "y_pred_selected = rf_reg_selected.predict(X_test[selected_features])\n",
        "\n",
        "# Calculate and print regression metrics for the model with selected features\n",
        "rmse_selected = sqrt(mean_squared_error(y_test, y_pred_selected))\n",
        "r2_selected = r2_score(y_test, y_pred_selected)\n",
        "\n",
        "print(f\"\\nRMSE with selected features: {rmse_selected:.5f}\")\n",
        "print(f\"R² with selected features: {r2_selected:.5f}\")"
      ],
      "metadata": {
        "id": "TI5cTVYqXlOr",
        "cellView": "form"
      },
      "id": "TI5cTVYqXlOr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code below plots the predicted vs actual values for the model with selected features."
      ],
      "metadata": {
        "id": "LBgFqig9Xqon"
      },
      "id": "LBgFqig9Xqon"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=y_test, y=y_pred_selected)\n",
        "plt.plot([0, 100], [0, 100], color='red', linestyle='--', label='1:1 Line')\n",
        "plt.xlim(0, 100)\n",
        "plt.ylim(0, 100)\n",
        "plt.xlabel('Actual Conifer Percentage')\n",
        "plt.ylabel('Predicted Conifer Percentage (Selected Features)')\n",
        "plt.title('Actual vs. Predicted Conifer Percentage (with Feature Selection)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r2L3PXkvXs4u"
      },
      "id": "r2L3PXkvXs4u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, while we've made progress, we haven't yet seen the significant performance improvement we hoped for. This often points to the need for further model optimization.\n",
        "\n",
        "A critical area to address is the hyperparameters of our model. These are settings that are configured before the training process begins, and their values can dramatically impact how well the model learns and performs. Manually testing an endless number of combinations is impractical.\n",
        "\n",
        "Fortunately, Python offers powerful tools to automatically search through and tune these hyperparameters. In the next section, we will explore the steps to leverage these automated methods, aiming to unlock a better-performing model."
      ],
      "metadata": {
        "id": "Xm-HR9U0mCUd"
      },
      "id": "Xm-HR9U0mCUd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal 4 - Experiment with different hyperparameters to limit overfitting"
      ],
      "metadata": {
        "id": "zc0Rwu3hX4jT"
      },
      "id": "zc0Rwu3hX4jT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's move on to experimenting with different hyperparameters to try and achieve a better-performing model. While this isn't a guaranteed solution for massive leaps in performance, it's often a relatively easy and highly recommended step that can yield noticeable improvements.\n",
        "\n",
        "Hyperparameters control the learning process and structure of our model (e.g., the number of trees in a Random Forest or the maximum depth of a decision tree). By systematically searching for the optimal combination of these settings, we can fine-tune our model to better fit the data and generalize more effectively. It's a powerful way to squeeze out that extra bit of performance, and the setup is usually quite straightforward in Python."
      ],
      "metadata": {
        "id": "CyLbEFO7gtLF"
      },
      "id": "CyLbEFO7gtLF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the code below:\n",
        "\n",
        "**What it does:**\n",
        "- Uses `selected_features` if it was created earlier (e.g., through feature selection).\n",
        "- Otherwise, defaults to all features in `features_ls`.\n",
        "\n",
        "**Why it's important:**\n",
        "- Helps avoid overfitting by using only relevant features.\n",
        "- Speeds up model training.\n",
        "- Ensures your model doesn't learn from irrelevant data.\n",
        "\n"
      ],
      "metadata": {
        "id": "cbyucKxeLAHJ"
      },
      "id": "cbyucKxeLAHJ"
    },
    {
      "cell_type": "code",
      "source": [
        "features_to_use = selected_features if 'selected_features' in locals() else features_ls\n",
        "print(f\"Starting hyperparameter tuning on {len(features_to_use)} features.\")"
      ],
      "metadata": {
        "id": "Q--Cquf_JnIq"
      },
      "id": "Q--Cquf_JnIq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use feature selection and hyperparamter tunning together rather than seperate, as it would always be best to do both of these things."
      ],
      "metadata": {
        "id": "h0rqvmhgwn7W"
      },
      "id": "h0rqvmhgwn7W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "`from sklearn.model_selection import GridSearchCV`\n",
        "\n",
        "This imports `GridSearchCV`, a hyperparameter tuning tool from scikit-learn.\n",
        "\n",
        "GridSearchCV searches for the best hyperparameter combination by:\n",
        "\n",
        " - Exhaustively trying every single combination of hyperparameters from a pre-defined grid of values.\n",
        "\n",
        "- Using cross-validation to evaluate model performance for each combination.\n",
        "\n",
        "- Guarantees finding the best combination within the defined grid, but can be computationally expensive when the parameter space is large.\n",
        "\n",
        "- Best suited for smaller parameter spaces or when you need to be certain you've found the optimal combination within your defined search range."
      ],
      "metadata": {
        "id": "fYL63iR4Lbnf"
      },
      "id": "fYL63iR4Lbnf"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "53ZS4GE_KeQ3"
      },
      "id": "53ZS4GE_KeQ3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    # Number of trees in the forest. Increasing this generally improves performance\n",
        "    # up to a certain point, after which returns diminish. Common values range from\n",
        "    # 100 to 200, balancing performance with computational cost.\n",
        "    'n_estimators': [100, 150, 200],\n",
        "\n",
        "    # The maximum depth of the tree. Limiting depth helps control overfitting.\n",
        "    # 'None' means nodes are expanded until all leaves are pure or contain\n",
        "    # less than min_samples_split samples.\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "\n",
        "    # The minimum number of samples required to split an internal node.\n",
        "    # Higher values prevent a model from learning relations which are highly specific\n",
        "    # to the particular training samples, thus reducing overfitting.\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "\n",
        "    # The minimum number of samples required to be at a leaf node.\n",
        "    # Similar to min_samples_split, higher values lead to more generalized models.\n",
        "    'min_samples_leaf': [1, 2, 5],\n",
        "\n",
        "    # The number of features to consider when looking for the best split.\n",
        "    # 'sqrt' (square root of total features) is a common and often effective choice\n",
        "    # for Random Forests, as it introduces randomness and reduces correlation\n",
        "    # between trees.\n",
        "    'max_features': ['sqrt'],\n",
        "\n",
        "    # Whether bootstrap samples are used when building trees.\n",
        "    # 'True' (default) means that each tree in the forest is built from a\n",
        "    # bootstrap sample of the training data, which is fundamental to\n",
        "    # the bagging ensemble method of Random Forests.\n",
        "    'bootstrap': [True]\n",
        "}"
      ],
      "metadata": {
        "id": "gEkenGTYiq3W"
      },
      "id": "gEkenGTYiq3W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 - fill in the code below to do hyperparameters tuning with GridSearch**"
      ],
      "metadata": {
        "id": "K-xsL4rhtd8g"
      },
      "id": "K-xsL4rhtd8g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **_Note:_** This code might take some time to finish running. It should take no more than 5 minutes."
      ],
      "metadata": {
        "id": "n1Y0KSm7h8FW"
      },
      "id": "n1Y0KSm7h8FW"
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestRegressor(random_state=25, oob_score=True)\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=...,\n",
        "    param_grid=...,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train[features_to_use], y_train)\n",
        "\n",
        "print(f\"\\nBest hyperparameters found by Faster RandomizedSearchCV: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation score (negative MSE): {grid_search.best_score_:.2f}\")"
      ],
      "metadata": {
        "id": "qR67xvT0T026"
      },
      "id": "qR67xvT0T026",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "rf = RandomForestRegressor(random_state=25, oob_score=True)\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train[features_to_use], y_train)\n",
        "\n",
        "print(f\"\\nBest hyperparameters found by Faster RandomizedSearchCV: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation score (negative MSE): {grid_search.best_score_:.2f}\")"
      ],
      "metadata": {
        "id": "Ac7U8zlYtWK4",
        "cellView": "form"
      },
      "id": "Ac7U8zlYtWK4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the code below:\n",
        "\n",
        "\n",
        "**What it does:**\n",
        "- Evaluates the best model's performance on unseen test data.\n",
        "- Reports RMSE and R² score.\n",
        "\n",
        "**Why it's important:**\n",
        "- Confirms whether your tuned model generalizes well.\n",
        "- RMSE tells you how far off predictions are on average.\n",
        "- R² indicates how much variance in the target is explained."
      ],
      "metadata": {
        "id": "rCHqfPSCLQ4P"
      },
      "id": "rCHqfPSCLQ4P"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best model\n",
        "best_grid_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set with the best model\n",
        "y_pred_grid = best_grid_model.predict(X_test[features_to_use])\n",
        "\n",
        "# Evaluate the best model\n",
        "rmse_grid = sqrt(mean_squared_error(y_test, y_pred_grid))\n",
        "r2_grid = r2_score(y_test, y_pred_grid)\n",
        "\n",
        "print(f\"\\nRMSE of the best model on test set: {rmse_grid:.5f}\")\n",
        "print(f\"R² of the best model on test set: {r2_grid:.5f}\")"
      ],
      "metadata": {
        "id": "enyI3fyAUyN_"
      },
      "id": "enyI3fyAUyN_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot below displays the predicted values against the actual values for our model, which was trained using the selected features. We can observe that the prediction line closely follows the general trend of the data points and exhibits a good fit, indicating that the model's performance in capturing the underlying relationship is satisfactory."
      ],
      "metadata": {
        "id": "mx9dWY31qTun"
      },
      "id": "mx9dWY31qTun"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting results for the best tuned model\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=y_test, y=y_pred_grid)\n",
        "plt.plot([0, 100], [0, 100], color='red', linestyle='--', label='1:1 Line')\n",
        "plt.xlim(0, 100)\n",
        "plt.ylim(0, 100)\n",
        "plt.xlabel('Actual Conifer Percentage')\n",
        "plt.ylabel('Predicted Conifer Percentage')\n",
        "plt.title('Actual vs. Predicted Conifer Percentage')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rfq0vCcXp5rB"
      },
      "id": "rfq0vCcXp5rB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a scoring metric for GridSearchCV, let's explore RandomizedSearchCV, another powerful search method that efficiently samples random hyperparameter combinations from specified distributions, often finding good solutions faster than GridSearchCV for large search spaces."
      ],
      "metadata": {
        "id": "waXFph5in8q9"
      },
      "id": "waXFph5in8q9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now looking at the code below:\n",
        "\n",
        "`from sklearn.model_selection import RandomizedSearchCV`\n",
        "\n",
        "This imports `RandomizedSearchCV`, a hyperparameter tuning tool from `scikit-learn`.\n",
        "\n",
        "`RandomizedSearchCV` searches for the best hyperparameter combination by:\n",
        "- Sampling a fixed number of **random combinations** from a specified distribution or list of parameters.\n",
        "- Using **cross-validation** to evaluate model performance for each sampled combination.\n",
        "\n",
        "- Faster than `GridSearchCV` when the parameter space is large.\n",
        "- Allows use of **continuous distributions** (e.g., `uniform`) and **discrete distributions** (e.g., `randint`).\n",
        "\n",
        "`from scipy.stats import randint, uniform`\n",
        "This imports random distribution generators from scipy.stats:\n",
        "\n",
        "- randint\n",
        "    - Generates random integers from a specified range (e.g., randint(10, 100) → 10 to 99).\n",
        "\n",
        "    - Used for hyperparameters that require integer values like n_estimators, max_depth, etc.\n",
        "\n",
        "- uniform\n",
        "    - Generates continuous float values from a uniform distribution over an interval.\n",
        "\n",
        "    - Useful for hyperparameters like learning_rate or regularization strength."
      ],
      "metadata": {
        "id": "Oea9KcYHZ13g"
      },
      "id": "Oea9KcYHZ13g"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint, uniform"
      ],
      "metadata": {
        "id": "SjPtl14FQX5X"
      },
      "id": "SjPtl14FQX5X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the code below:\n",
        "\n",
        "**What it does:**\n",
        "- Prepares a randomized search over the defined parameter space.\n",
        "\n",
        "**Why it's important:**\n",
        "- Tunes multiple hyperparameters in parallel.\n",
        "- Uses cross-validation to evaluate each configuration reliably.\n",
        "- Reduces training time compared to `GridSearchCV`.\n"
      ],
      "metadata": {
        "id": "9TrHL30jLMhy"
      },
      "id": "9TrHL30jLMhy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter distributions to sample from for a faster Randomized Search\n",
        "param_grid_random = {\n",
        "    # Number of trees in the forest. Using randint for RandomSearchCV allows the\n",
        "    # search to sample a wide, continuous range, providing more flexibility than\n",
        "    # discrete values. Here, it samples uniformly from 100 up to (but not including) 200.\n",
        "    'n_estimators': randint(100, 200),\n",
        "\n",
        "    # The maximum depth of the tree. Similar to n_estimators, using randint for depth\n",
        "    # enables the random search to explore a broader spectrum of tree complexities,\n",
        "    # from shallow (10) to relatively deep (up to, but not including, 30).\n",
        "    'max_depth': randint(10, 30),\n",
        "\n",
        "    # The minimum number of samples required to split an internal node.\n",
        "    # Random sampling within this range helps in finding an optimal balance\n",
        "    # between model complexity and preventing overfitting on specific training examples.\n",
        "    'min_samples_split': randint(2, 10),\n",
        "\n",
        "    # The minimum number of samples required to be at a leaf node.\n",
        "    # Randomly sampling this parameter helps in exploring various levels of\n",
        "    # model generalization, from highly specific (1 sample) to more generalized (up to 5 samples).\n",
        "    'min_samples_leaf': randint(1, 5),\n",
        "\n",
        "    # The number of features to consider when looking for the best split.\n",
        "    # 'sqrt' (square root of total features) is a common and often effective choice\n",
        "    # for Random Forests, as it introduces randomness and reduces correlation\n",
        "    # between trees. This is kept as a fixed categorical choice, as typically\n",
        "    # a few well-known strategies (like 'sqrt', 'log2', or float fractions) are sufficient.\n",
        "    'max_features': ['sqrt'],\n",
        "\n",
        "    # Defines whether bootstrap samples are used when building trees.\n",
        "    # 'True' (default) means that each tree in the forest is built from a\n",
        "    # bootstrap sample of the training data, which is fundamental to\n",
        "    # the bagging ensemble method of Random Forests. This is usually kept\n",
        "    # fixed as 'True' for a standard Random Forest.\n",
        "    'bootstrap': [True]\n",
        "}"
      ],
      "metadata": {
        "id": "duQou1pLJnV8"
      },
      "id": "duQou1pLJnV8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the code below:\n",
        "\n",
        "**What it does:**\n",
        "- Trains multiple versions of the model using different parameter combinations.\n",
        "- Selects the best-performing model based on cross-validation score.\n",
        "\n",
        "**Why it's important:**\n",
        "- Finds the best model configuration to improve accuracy and generalization.\n",
        "- Automates tuning, reducing guesswork.\n",
        "\n",
        "**Question 2 - fill in the code below.**\n",
        "\n",
        "> **_Note:_** This code might take some time to finish running. It should take no more than 5 minutes."
      ],
      "metadata": {
        "id": "nBrsCvkgLO09"
      },
      "id": "nBrsCvkgLO09"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize RandomForestRegressor\n",
        "# Use OOB score for internal validation and regularization benefit\n",
        "rf_fast = ...(random_state=25, oob_score=True, bootstrap=True)\n",
        "\n",
        "# Improve scoring metric to RMSE instead of MSE (same optimization direction but more interpretable)\n",
        "random_search_fast = ...(\n",
        "    estimator=...,\n",
        "    param_distributions=...,\n",
        "    n_iter=60,\n",
        "    cv=5,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    random_state=25,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data using the selected features\n",
        "random_search_fast....(X_train[features_to_use], y_train)\n",
        "\n",
        "print(f\"\\nBest hyperparameters found by Faster RandomizedSearchCV: {random_search_fast.best_params_}\")\n",
        "print(f\"Best cross-validation score (negative MSE): {random_search_fast.best_score_:.2f}\")"
      ],
      "metadata": {
        "id": "MDAFvN-2Jnbr"
      },
      "id": "MDAFvN-2Jnbr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "# Initialize RandomForestRegressor\n",
        "# Use OOB score for internal validation and regularization benefit\n",
        "rf_fast = RandomForestRegressor(random_state=25, oob_score=True, bootstrap=True)\n",
        "\n",
        "# Improve scoring metric to RMSE instead of MSE (same optimization direction but more interpretable)\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf_fast,\n",
        "    param_distributions=param_grid_random,\n",
        "    n_iter=60,\n",
        "    cv=5,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    random_state=25,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data using the selected features\n",
        "random_search.fit(X_train[features_to_use], y_train)\n",
        "\n",
        "print(f\"\\nBest hyperparameters found by Faster RandomizedSearchCV: {random_search.best_params_}\")\n",
        "print(f\"Best cross-validation score (negative MSE): {random_search.best_score_:.2f}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hiSB6Q93YIbu"
      },
      "id": "hiSB6Q93YIbu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best model\n",
        "best_random_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set with the best model\n",
        "y_pred_random = random_search.predict(X_test[features_to_use])\n",
        "\n",
        "# Evaluate the best model\n",
        "rmse_random = sqrt(mean_squared_error(y_test, y_pred_random))\n",
        "r2_random = r2_score(y_test, y_pred_random)\n",
        "\n",
        "print(f\"\\nRMSE of the best model on test set: {rmse_random:.5f}\")\n",
        "print(f\"R² of the best model on test set: {r2_random:.5f}\")"
      ],
      "metadata": {
        "id": "D-Ptr2bOJyHW"
      },
      "id": "D-Ptr2bOJyHW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot below displays the predicted values against the actual values for our model, which was trained using the selected features. We can observe that the prediction line closely follows the general trend of the data points and exhibits a good fit, indicating that the model's performance in capturing the underlying relationship is satisfactory."
      ],
      "metadata": {
        "id": "JDAdEtZNrBCF"
      },
      "id": "JDAdEtZNrBCF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting results for the best tuned model\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=y_test, y=y_pred_random)\n",
        "plt.plot([0, 100], [0, 100], color='red', linestyle='--', label='1:1 Line')\n",
        "plt.xlim(0, 100)\n",
        "plt.ylim(0, 100)\n",
        "plt.xlabel('Actual Conifer Percentage')\n",
        "plt.ylabel('Predicted Conifer Percentage (Faster Tuned Model)')\n",
        "plt.title('Actual vs. Predicted Conifer Percentage (Faster RandomizedSearchCV Tuned)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "87bs2EchYWdH"
      },
      "id": "87bs2EchYWdH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To synthesize our findings and illustrate the results of our modeling approaches, the table below offers a side-by-side comparison of key performance metrics (RMSE and R²) for each experiment conducted in this tutorial. Graphs are omitted due to minimal variation between them."
      ],
      "metadata": {
        "id": "-_ppbbFxsLc4"
      },
      "id": "-_ppbbFxsLc4"
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\n",
        "    'Metric': ['RMSE', 'R²'],\n",
        "    'Baseline Model': [rmse, r2],\n",
        "    'Feature Selection': [rmse_selected, r2_selected],\n",
        "    'Randomized Search': [rmse_random, r2_random],\n",
        "    'Grid Search': [rmse_grid, r2_grid]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"Experiment Results Comparison:\")\n",
        "print(results_df.round(5).to_string(index=False))"
      ],
      "metadata": {
        "id": "EK8l0j7CsBef"
      },
      "id": "EK8l0j7CsBef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3 - What are some conclusions which you can draw from this result?**"
      ],
      "metadata": {
        "id": "e_gQk7bisCYD"
      },
      "id": "e_gQk7bisCYD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer here*"
      ],
      "metadata": {
        "id": "4X_DaYgIvm0Q"
      },
      "id": "4X_DaYgIvm0Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details open>\n",
        "<summary>Solution</summary>\n",
        "While Grid Search yielded the best results, the overall performance improvement across all methods is relatively modest. The change in RMSE is only about 0.25 (from ~15.25 to ~15.00) and R² increased by less than 1% (from ~0.773 to ~0.781). This suggests that while hyperparameter tuning helped refine the model, there might be inherent limitations in the dataset, the chosen model type, or further pre-processing steps (like more advanced feature engineering) that could lead to more substantial gains.\n",
        "details\n",
        "</details>"
      ],
      "metadata": {
        "id": "j-xQ97gTwLcM"
      },
      "id": "j-xQ97gTwLcM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This experiment reinforced that achieving accurate models with decision trees and random forests is contingent on correct tuning. We found that hyperparameter and feature selection offered slight performance enhancements in our specific case, but recognizing their potential for substantial impact, they remain common and recommended practices for model optimization across various situations."
      ],
      "metadata": {
        "id": "q2uN_3_ifamc"
      },
      "id": "q2uN_3_ifamc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References"
      ],
      "metadata": {
        "id": "3SFEnaIuk6Pm"
      },
      "id": "3SFEnaIuk6Pm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemini. (2025). Assistance is editing writeups and code. Retrieved from https://gemini.google.com"
      ],
      "metadata": {
        "id": "LuQ9RAF3kytv"
      },
      "id": "LuQ9RAF3kytv"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "petawawa-preprocessing",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}